{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-LSTM Grade Classifier\n",
    "\n",
    "In this notebook we will train a Bi-LSTM to classify moonboard problems by grade. We also experiment with a Word2Vec based hold embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from file:\n",
    "with open('problems.json', 'r') as fp:\n",
    "    problems_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "problem_name     0\n",
       "info             0\n",
       "url              0\n",
       "moves           22\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process data\n",
    "df = pd.DataFrame.from_dict(problems_dict, orient = 'index')\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14902"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hold_index(d):\n",
    "    # convert a move to the coorindates of the hold on the board\n",
    "    s_split = re.split('(\\d+)', d['Description'], maxsplit=1)\n",
    "    # extra `-1` in both for 0 indexing\n",
    "    w = ord(s_split[0].upper()) - 64 - 1\n",
    "    h = int(s_split[1]) - 1\n",
    "    \n",
    "    #return 11*18-(11*h+w)\n",
    "    return 11*h+w\n",
    "\n",
    "\n",
    "def get_sequence(moves):\n",
    "    seq = []\n",
    "    \n",
    "    for move in moves:\n",
    "        seq.append(hold_index(move))\n",
    "    return seq\n",
    "\n",
    "\n",
    "df['Move sequence'] = df['moves'].apply(get_sequence)\n",
    "problems = list(df['Move sequence'])\n",
    "len(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14902"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process labels\n",
    "grades = []\n",
    "for problem in df['info']:\n",
    "    grades.append(problem[2])\n",
    "    \n",
    "grade_map = {\n",
    "        '5+':0,\n",
    "        '6A': 1,\n",
    "        '6A+': 2,\n",
    "        '6B': 3,\n",
    "        '6B+': 4,\n",
    "        '6C': 5,\n",
    "        '6C+': 6,\n",
    "        '7A': 7,\n",
    "        '7A+': 8,\n",
    "        '7B': 9,\n",
    "        '7B+': 10,\n",
    "        '7C': 11,\n",
    "        '7C+': 12,\n",
    "        '8A': 13,\n",
    "        '8A+': 14,\n",
    "        '8B': 15,\n",
    "        '8B+': 16,\n",
    "        '8C': 17,\n",
    "        '8C+': 18\n",
    "    }\n",
    "\n",
    "grades = [grade.split()[0] for grade in grades]\n",
    "grades = [grade_map[grade] for grade in grades]\n",
    "len(grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(problems, grades, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        \n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
    "        #self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim//2, 19)\n",
    "\n",
    "        \n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
    "\n",
    "    \n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embeds, self.hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        \n",
    "        fbhn = (h_n[-2,:,:]+h_n[-1,:,:])\n",
    "        \n",
    "        lstm_feats = self.hidden2tag(fbhn)\n",
    "        return lstm_feats\n",
    "\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        \n",
    "        return lstm_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiLSTM(11*18, 20, 50).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Training loss: 19784.21, Training acc: 0.26, val loss: 6200.07, val acc: 0.27, time: 47.90s\n",
      "Epoch:2, Training loss: 18012.17, Training acc: 0.29, val loss: 5950.08, val acc: 0.29, time: 48.24s\n",
      "Epoch:3, Training loss: 17387.26, Training acc: 0.31, val loss: 5861.43, val acc: 0.29, time: 47.99s\n",
      "Epoch:4, Training loss: 17045.92, Training acc: 0.32, val loss: 5818.35, val acc: 0.30, time: 48.19s\n",
      "Epoch:5, Training loss: 16802.72, Training acc: 0.32, val loss: 5793.82, val acc: 0.30, time: 48.03s\n",
      "Epoch:6, Training loss: 16632.71, Training acc: 0.33, val loss: 5796.31, val acc: 0.30, time: 49.86s\n",
      "Epoch:7, Training loss: 16465.19, Training acc: 0.33, val loss: 5789.23, val acc: 0.30, time: 48.09s\n",
      "Epoch:8, Training loss: 16302.94, Training acc: 0.34, val loss: 5797.99, val acc: 0.30, time: 48.14s\n",
      "Epoch:9, Training loss: 16149.53, Training acc: 0.35, val loss: 5807.84, val acc: 0.30, time: 48.43s\n",
      "Epoch:10, Training loss: 16014.88, Training acc: 0.35, val loss: 5801.75, val acc: 0.31, time: 47.87s\n",
      "Epoch:11, Training loss: 15869.01, Training acc: 0.36, val loss: 5835.25, val acc: 0.31, time: 47.91s\n",
      "Epoch:12, Training loss: 15738.75, Training acc: 0.36, val loss: 5858.75, val acc: 0.31, time: 48.00s\n",
      "Epoch:13, Training loss: 15585.63, Training acc: 0.36, val loss: 5877.55, val acc: 0.30, time: 47.82s\n",
      "Epoch:14, Training loss: 15472.87, Training acc: 0.37, val loss: 5911.72, val acc: 0.30, time: 49.90s\n",
      "Epoch:15, Training loss: 15309.40, Training acc: 0.38, val loss: 5944.93, val acc: 0.30, time: 48.00s\n",
      "Epoch:16, Training loss: 15190.29, Training acc: 0.39, val loss: 5993.25, val acc: 0.29, time: 48.12s\n",
      "Epoch:17, Training loss: 15034.91, Training acc: 0.40, val loss: 6002.90, val acc: 0.29, time: 48.32s\n",
      "Epoch:18, Training loss: 14907.90, Training acc: 0.40, val loss: 6054.09, val acc: 0.30, time: 49.89s\n",
      "Epoch:19, Training loss: 14738.20, Training acc: 0.41, val loss: 6123.52, val acc: 0.29, time: 52.50s\n",
      "Epoch:20, Training loss: 14596.22, Training acc: 0.41, val loss: 6176.08, val acc: 0.29, time: 49.83s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Each epoch will take about 1-2 minutes\"\"\"\n",
    "\n",
    "import datetime\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in range(20):  \n",
    "    time1 = datetime.datetime.now()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    model.train()\n",
    "    for i, idxs in enumerate(X_train):\n",
    "        grade = y_train[i]\n",
    "\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is,\n",
    "        # turn them into Tensors of problem hold indices.\n",
    "        problem_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
    "        grade = torch.tensor([grade], dtype=torch.long).to(device)\n",
    "        \n",
    "        p = model(problem_in)\n",
    "        _, pred = torch.max(p, 1)\n",
    "        if pred == grade:\n",
    "            train_correct +=1\n",
    "        \n",
    "        \n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        loss = criterion(p, grade)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss+=loss.item()\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    for i, idxs in enumerate(X_val):\n",
    "        grade = y_val[i]\n",
    "        problem_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
    "        grade = torch.tensor([grade], dtype=torch.long).to(device)\n",
    "        p = model(problem_in)\n",
    "        _, pred = torch.max(p, 1)\n",
    "        if pred == grade:\n",
    "            val_correct +=1\n",
    "        loss = criterion(p, grade)\n",
    "        val_loss+=loss.item()\n",
    "    \n",
    "    time2 = datetime.datetime.now()\n",
    "\n",
    "    print(\"Epoch:%d, Training loss: %.2f, Training acc: %.2f, val loss: %.2f, val acc: %.2f, time: %.2fs\" %(epoch+1, train_loss, train_correct/len(X_train), val_loss, val_correct/len(X_val), (time2-time1).total_seconds()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hold Embedding\n",
    "\n",
    "We train a word2vec model to obtain a vector representation of each hold on the Moonboard. We regard each problem as a sentence where each word in the sentence is a hold on the moonboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_string = []\n",
    "for seq in problems:\n",
    "    string_seq = [str(hold) for hold in seq]\n",
    "    problem_string.append(string_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a gensim word2vec model\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "\n",
    "\n",
    "# Callback class used for reporting loss during training process\n",
    "class epochLoss(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.epoch = 1\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 1:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        else:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss- self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 102076.3203125\n",
      "Loss after epoch 2: 89125.1953125\n",
      "Loss after epoch 3: 88068.359375\n",
      "Loss after epoch 4: 87429.28125\n",
      "Loss after epoch 5: 86599.25\n",
      "Loss after epoch 6: 87019.65625\n",
      "Loss after epoch 7: 86874.4375\n",
      "Loss after epoch 8: 116645.0625\n",
      "Loss after epoch 9: 87252.1875\n",
      "Loss after epoch 10: 87123.875\n",
      "Loss after epoch 11: 87192.0\n",
      "Loss after epoch 12: 87564.25\n",
      "Loss after epoch 13: 87374.75\n",
      "Loss after epoch 14: 88479.75\n",
      "Loss after epoch 15: 87544.875\n",
      "Loss after epoch 16: 87516.75\n",
      "Loss after epoch 17: 87109.125\n",
      "Loss after epoch 18: 87184.125\n",
      "Loss after epoch 19: 87008.75\n",
      "Loss after epoch 20: 116666.625\n",
      "Loss after epoch 21: 86903.625\n",
      "Loss after epoch 22: 86375.625\n",
      "Loss after epoch 23: 86655.75\n",
      "Loss after epoch 24: 84640.625\n",
      "Loss after epoch 25: 84359.75\n",
      "Loss after epoch 26: 85577.5\n",
      "Loss after epoch 27: 84979.75\n",
      "Loss after epoch 28: 84451.25\n",
      "Loss after epoch 29: 84311.25\n",
      "Loss after epoch 30: 84656.75\n",
      "Loss after epoch 31: 84381.0\n",
      "Loss after epoch 32: 83267.25\n",
      "Loss after epoch 33: 83442.0\n",
      "Loss after epoch 34: 84490.0\n",
      "Loss after epoch 35: 85400.5\n",
      "Loss after epoch 36: 83743.75\n",
      "Loss after epoch 37: 83166.75\n",
      "Loss after epoch 38: 83592.5\n",
      "Loss after epoch 39: 112989.5\n",
      "Loss after epoch 40: 84455.5\n",
      "Loss after epoch 41: 84828.5\n",
      "Loss after epoch 42: 83295.25\n",
      "Loss after epoch 43: 83036.75\n",
      "Loss after epoch 44: 83574.5\n",
      "Loss after epoch 45: 112720.0\n",
      "Loss after epoch 46: 85203.0\n",
      "Loss after epoch 47: 85231.25\n",
      "Loss after epoch 48: 102588.5\n",
      "Loss after epoch 49: 74026.5\n",
      "Loss after epoch 50: 74573.0\n"
     ]
    }
   ],
   "source": [
    "#train word embedding model\n",
    "model = Word2Vec(sentences=problem_string, size=20, window=2, min_count=5, workers=4, sg=1, iter = 50, compute_loss = True, callbacks=[epochLoss()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "holds = np.arange(11*18)\n",
    "holds_list = [str(hold) for hold in holds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198, 20)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "EMBEDDING_DIM = 20\n",
    "\n",
    "embedding_matrix = []\n",
    "for hold in holds_list:\n",
    "    try:\n",
    "        embedding_matrix.append(model.wv[hold])\n",
    "    except:\n",
    "        embedding_matrix.append([0]*EMBEDDING_DIM)\n",
    "embedding_matrix = np.array(embedding_matrix)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use hold embedding as initial weights for nn.Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        \n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
    "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim//2, 19)\n",
    "\n",
    "        \n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2).to(device),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2).to(device))\n",
    "\n",
    "    \n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embeds, self.hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        \n",
    "        fbhn = (h_n[-2,:,:]+h_n[-1,:,:])\n",
    "        \n",
    "        lstm_feats = self.hidden2tag(fbhn)\n",
    "        return lstm_feats\n",
    "\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        \n",
    "        return lstm_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiLSTM(11*18, 20, 50).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Training loss: 18888.57, Training acc: 0.29, val loss: 5879.84, val acc: 0.29, time: 48.39s\n",
      "Epoch:2, Training loss: 17425.93, Training acc: 0.31, val loss: 5753.45, val acc: 0.30, time: 48.70s\n",
      "Epoch:3, Training loss: 17040.05, Training acc: 0.32, val loss: 5704.93, val acc: 0.30, time: 48.17s\n",
      "Epoch:4, Training loss: 16864.96, Training acc: 0.32, val loss: 5662.48, val acc: 0.31, time: 49.01s\n",
      "Epoch:5, Training loss: 16708.41, Training acc: 0.33, val loss: 5655.26, val acc: 0.31, time: 50.81s\n",
      "Epoch:6, Training loss: 16618.48, Training acc: 0.33, val loss: 5641.88, val acc: 0.31, time: 49.66s\n",
      "Epoch:7, Training loss: 16519.97, Training acc: 0.33, val loss: 5629.88, val acc: 0.31, time: 48.70s\n",
      "Epoch:8, Training loss: 16420.18, Training acc: 0.34, val loss: 5627.28, val acc: 0.31, time: 48.63s\n",
      "Epoch:9, Training loss: 16336.50, Training acc: 0.34, val loss: 5613.63, val acc: 0.31, time: 50.24s\n",
      "Epoch:10, Training loss: 16228.78, Training acc: 0.34, val loss: 5598.23, val acc: 0.31, time: 48.91s\n",
      "Epoch:11, Training loss: 16130.28, Training acc: 0.34, val loss: 5608.38, val acc: 0.31, time: 49.33s\n",
      "Epoch:12, Training loss: 16022.90, Training acc: 0.34, val loss: 5604.83, val acc: 0.31, time: 48.50s\n",
      "Epoch:13, Training loss: 15958.82, Training acc: 0.35, val loss: 5575.97, val acc: 0.31, time: 48.98s\n",
      "Epoch:14, Training loss: 15871.34, Training acc: 0.35, val loss: 5589.39, val acc: 0.31, time: 48.86s\n",
      "Epoch:15, Training loss: 15762.20, Training acc: 0.35, val loss: 5586.37, val acc: 0.32, time: 49.60s\n",
      "Epoch:16, Training loss: 15696.37, Training acc: 0.36, val loss: 5590.16, val acc: 0.31, time: 48.03s\n",
      "Epoch:17, Training loss: 15631.97, Training acc: 0.36, val loss: 5586.06, val acc: 0.32, time: 48.32s\n",
      "Epoch:18, Training loss: 15542.34, Training acc: 0.36, val loss: 5602.18, val acc: 0.31, time: 48.13s\n",
      "Epoch:19, Training loss: 15475.31, Training acc: 0.37, val loss: 5609.67, val acc: 0.31, time: 47.89s\n",
      "Epoch:20, Training loss: 15397.42, Training acc: 0.37, val loss: 5613.00, val acc: 0.32, time: 49.10s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Each epoch will take about 1-2 minutes\"\"\"\n",
    "\n",
    "import datetime\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for epoch in range(20):  \n",
    "    time1 = datetime.datetime.now()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    model.train()\n",
    "    for i, idxs in enumerate(X_train):\n",
    "        grade = y_train[i]\n",
    "\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is,\n",
    "        # turn them into Tensors of problem hold indices.\n",
    "        problem_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
    "        grade = torch.tensor([grade], dtype=torch.long).to(device)\n",
    "        \n",
    "        p = model(problem_in)\n",
    "        _, pred = torch.max(p, 1)\n",
    "        if pred == grade:\n",
    "            train_correct +=1\n",
    "        \n",
    "        \n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        loss = criterion(p, grade)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss+=loss.item()\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    for i, idxs in enumerate(X_val):\n",
    "        grade = y_val[i]\n",
    "        problem_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
    "        grade = torch.tensor([grade], dtype=torch.long).to(device)\n",
    "        p = model(problem_in)\n",
    "        _, pred = torch.max(p, 1)\n",
    "        if pred == grade:\n",
    "            val_correct +=1\n",
    "        loss = criterion(p, grade)\n",
    "        val_loss+=loss.item()\n",
    "    \n",
    "    time2 = datetime.datetime.now()\n",
    "\n",
    "    print(\"Epoch:%d, Training loss: %.2f, Training acc: %.2f, val loss: %.2f, val acc: %.2f, time: %.2fs\" %(epoch+1, train_loss, train_correct/len(X_train), val_loss, val_correct/len(X_val), (time2-time1).total_seconds()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
