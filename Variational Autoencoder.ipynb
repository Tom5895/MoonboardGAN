{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto-Encoder Generated Problems\n",
    "\n",
    "Auto-Encoder code adapted from https://github.com/atinghosh/VAE-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from file:\n",
    "with open('problems.json', 'r') as fp:\n",
    "    problems_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "problem_name     0\n",
       "info             0\n",
       "url              0\n",
       "moves           22\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process data\n",
    "df = pd.DataFrame.from_dict(problems_dict, orient = 'index')\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14902"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def move_coordinate(d):\n",
    "    # convert a move to the coorindates of the hold on the board\n",
    "    s_split = re.split('(\\d+)', d['Description'], maxsplit=1)\n",
    "    # extra `-1` in both for 0 indexing\n",
    "    w = ord(s_split[0].upper()) - 64 - 1\n",
    "    h = int(s_split[1]) - 1\n",
    "    \n",
    "    return (h, w, 0)\n",
    "\n",
    "\n",
    "def convert_moves(moves):\n",
    "    array = np.zeros((18, 11, 1))\n",
    "    for move in moves:\n",
    "        array[move_coordinate(move)] = 1\n",
    "    return array\n",
    "\n",
    "\n",
    "df['Moves_array'] = df['moves'].apply(convert_moves)\n",
    "problems = list(df['Moves_array'])\n",
    "len(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = np.moveaxis(problems, -1, 1)\n",
    "problems = problems.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14902"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process labels\n",
    "grades = []\n",
    "for problem in df['info']:\n",
    "    grades.append(problem[2])\n",
    "    \n",
    "grade_map = {\n",
    "        '5+':0,\n",
    "        '6A': 1,\n",
    "        '6A+': 2,\n",
    "        '6B': 3,\n",
    "        '6B+': 4,\n",
    "        '6C': 5,\n",
    "        '6C+': 6,\n",
    "        '7A': 7,\n",
    "        '7A+': 8,\n",
    "        '7B': 9,\n",
    "        '7B+': 10,\n",
    "        '7C': 11,\n",
    "        '7C+': 12,\n",
    "        '8A': 13,\n",
    "        '8A+': 14,\n",
    "        '8B': 15,\n",
    "        '8B+': 16,\n",
    "        '8C': 17,\n",
    "        '8C+': 18\n",
    "    }\n",
    "\n",
    "grades = [grade.split()[0] for grade in grades]\n",
    "grades = [grade_map[grade] for grade in grades]\n",
    "len(grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProblemDataset(Dataset):\n",
    "    def __init__(self, data, label, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.label = label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        image_tensor = torch.from_numpy(image)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        label = self.label[idx]\n",
    "        \n",
    "        \n",
    "        target = torch.tensor(label,dtype=torch.long)\n",
    "        \n",
    "        return image_tensor, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(problems, grades, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ProblemDataset(X_train, y_train)\n",
    "val = ProblemDataset(X_val, y_val)\n",
    "test = ProblemDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=16)\n",
    "val_loader = DataLoader(val, batch_size=16)\n",
    "test_loader = DataLoader(test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "# changed configuration to this instead of argparse for easier interaction\n",
    "CUDA = True\n",
    "SEED = 1\n",
    "BATCH_SIZE = 16\n",
    "LOG_INTERVAL = 10\n",
    "EPOCHS = 100\n",
    "no_of_sample = 10\n",
    "\n",
    "# connections through the autoencoder bottleneck\n",
    "# in the pytorch VAE example, this is 20\n",
    "ZDIMS = 20\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(4, 4), padding=(10, 7),\n",
    "                               stride=2)  # This padding keeps the size of the image same, i.e. same padding\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(4, 4), padding=(10, 7), stride=2)\n",
    "        self.fc11 = nn.Linear(in_features=128 * 18 * 11, out_features=1024)\n",
    "        self.fc12 = nn.Linear(in_features=1024, out_features=ZDIMS)\n",
    "\n",
    "        self.fc21 = nn.Linear(in_features=128 * 18 * 11, out_features=1024)\n",
    "        self.fc22 = nn.Linear(in_features=1024, out_features=ZDIMS)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # For decoder\n",
    "\n",
    "        # For mu\n",
    "        self.fc1 = nn.Linear(in_features=20, out_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=7 * 7 * 128)\n",
    "        self.conv_t1 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, padding=1, stride=2)\n",
    "        self.conv_t2 = nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=(7,2), padding=(1,2), stride=1)\n",
    "\n",
    "\n",
    "\n",
    "    def encode(self, x: Variable) -> (Variable, Variable):\n",
    "\n",
    "        x = x.view(-1, 1, 18, 11)\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = x.view(-1, 128 * 18 * 11)\n",
    "\n",
    "        mu_z = F.elu(self.fc11(x))\n",
    "        mu_z = self.fc12(mu_z)\n",
    "\n",
    "        logvar_z = F.elu(self.fc21(x))\n",
    "        logvar_z = self.fc22(logvar_z)\n",
    "\n",
    "        return mu_z, logvar_z\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu: Variable, logvar: Variable) -> Variable:\n",
    "\n",
    "\n",
    "        if self.training:\n",
    "            # multiply log variance with 0.5, then in-place exponent\n",
    "            # yielding the standard deviation\n",
    "\n",
    "            sample_z = []\n",
    "            for _ in range(no_of_sample):\n",
    "                std = logvar.mul(0.5).exp_()  # type: Variable\n",
    "                eps = Variable(std.data.new(std.size()).normal_())\n",
    "                sample_z.append(eps.mul(std).add_(mu))\n",
    "\n",
    "            return sample_z\n",
    "\n",
    "        else:\n",
    "            # During inference, we simply spit out the mean of the\n",
    "            # learned distribution for the current input.  We could\n",
    "            # use a random sample from the distribution, but mu of\n",
    "            # course has the highest probability.\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z: Variable) -> Variable:\n",
    "\n",
    "        x = F.elu(self.fc1(z))\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = x.view(-1, 128, 7, 7)\n",
    "        x = F.relu(self.conv_t1(x))\n",
    "        x = F.sigmoid(self.conv_t2(x))\n",
    "        return x.view(-1, 198)\n",
    "\n",
    "\n",
    "    def forward(self, x: Variable) -> (Variable, Variable, Variable):\n",
    "        mu, logvar = self.encode(x.view(-1, 198))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        if self.training:\n",
    "            return [self.decode(z) for z in z], mu, logvar\n",
    "        else:\n",
    "            return self.decode(z), mu, logvar\n",
    "        # return self.decode(z), mu, logvar\n",
    "\n",
    "    def loss_function(self, recon_x, x, mu, logvar) -> Variable:\n",
    "        # how well do input x and output recon_x agree?\n",
    "\n",
    "        if self.training:\n",
    "            BCE = 0\n",
    "            for recon_x_one in recon_x:\n",
    "                BCE += F.binary_cross_entropy(recon_x_one, x.view(-1, 198))\n",
    "            BCE /= len(recon_x)\n",
    "        else:\n",
    "            BCE = F.binary_cross_entropy(recon_x, x.view(-1, 198))\n",
    "\n",
    "        # KLD is Kullbackâ€“Leibler divergence -- how much does one learned\n",
    "        # distribution deviate from another, in this specific case the\n",
    "        # learned distribution from the unit Gaussian\n",
    "\n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # https://arxiv.org/abs/1312.6114\n",
    "        # - D_{KL} = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        # note the negative D_{KL} in appendix B of the paper\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        # Normalise by same number of elements as in reconstruction\n",
    "        KLD /= BATCH_SIZE * 198\n",
    "\n",
    "\n",
    "        return BCE + KLD\n",
    "\n",
    "\n",
    "model = VAE()\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    # toggle model to train mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    # in the case of MNIST, len(train_loader.dataset) is 60000\n",
    "    # each `data` is of BATCH_SIZE samples and has shape [128, 1, 28, 28]\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        \n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # push whole batch of data through VAE.forward() to get recon_loss\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        # calculate scalar loss\n",
    "        loss = model.loss_function(recon_batch, data, mu, logvar)\n",
    "        # calculate the gradient of the loss w.r.t. the graph leaves\n",
    "        # i.e. input variables -- by the power of pytorch!\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader),\n",
    "                       loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    # each data is of BATCH_SIZE (default 128) samples\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        if CUDA:\n",
    "            # make sure this lives on the GPU\n",
    "            data = data.cuda()\n",
    "\n",
    "        # we're only going to infer, so no autograd at all required: volatile=True\n",
    "        data = Variable(data, volatile=True)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += model.loss_function(recon_batch, data, mu, logvar).item()\n",
    "        if i == 0:\n",
    "            n = min(data.size(0), 8)\n",
    "            # for the first 128 batch of the epoch, show the first 8 input digits\n",
    "            # with right below them the reconstructed output digits\n",
    "            comparison = torch.cat([data[:n],\n",
    "                                    recon_batch.view(BATCH_SIZE, 1, 18, 11)[:n]])\n",
    "            save_image(comparison.data.cpu(),\n",
    "                       'reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "\n",
    "        # 64 sets of random ZDIMS-float vectors\n",
    "        sample = Variable(torch.randn(64, ZDIMS))\n",
    "        if CUDA:\n",
    "            sample = sample.cuda()\n",
    "        sample = model.decode(sample).cpu()\n",
    "\n",
    "        # save out as an 8x8 matrix of generated problems\n",
    "        # this will give you a visual idea of how well latent space can generate things\n",
    "        # that look like problems\n",
    "        save_image(sample.data.view(64, 1, 18, 11),'reconstruction' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show an image of a generated problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = Variable(torch.randn(1, ZDIMS))\n",
    "if CUDA:\n",
    "    sample = sample.cuda()\n",
    "sample = model.decode(sample).cpu()\n",
    "\n",
    "        # save out as an 8x8 matrix of MNIST digits\n",
    "        # this will give you a visual idea of how well latent space can generate things\n",
    "        # that look like digits\n",
    "save_image(sample.data.view(1, 1, 18, 11),'test' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = sample.data[0].view(18,11).numpy()\n",
    "pic2 = np.flipud(pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(pic2>0.21, cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
