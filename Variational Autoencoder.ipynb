{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto-Encoder Generated Problems\n",
    "\n",
    "Auto-Encoder code adapted from https://github.com/atinghosh/VAE-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from file:\n",
    "with open(r'C:\\Users\\Tom\\Documents\\Untitled Folder\\problems.json', 'r') as fp:\n",
    "    problems_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "problem_name     0\n",
       "info             0\n",
       "url              0\n",
       "moves           22\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process data\n",
    "df = pd.DataFrame.from_dict(problems_dict, orient = 'index')\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14902"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def move_coordinate(d):\n",
    "    # convert a move to the coorindates of the hold on the board\n",
    "    s_split = re.split('(\\d+)', d['Description'], maxsplit=1)\n",
    "    # extra `-1` in both for 0 indexing\n",
    "    w = ord(s_split[0].upper()) - 64 - 1\n",
    "    h = int(s_split[1]) - 1\n",
    "    \n",
    "    return (h, w, 0)\n",
    "\n",
    "\n",
    "def convert_moves(moves):\n",
    "    array = np.zeros((18, 11, 1))\n",
    "    for move in moves:\n",
    "        array[move_coordinate(move)] = 1\n",
    "    return array\n",
    "\n",
    "\n",
    "df['Moves_array'] = df['moves'].apply(convert_moves)\n",
    "problems = list(df['Moves_array'])\n",
    "len(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = np.moveaxis(problems, -1, 1)\n",
    "problems = problems.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14902"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process labels\n",
    "grades = []\n",
    "for problem in df['info']:\n",
    "    grades.append(problem[2])\n",
    "    \n",
    "grade_map = {\n",
    "        '5+':0,\n",
    "        '6A': 1,\n",
    "        '6A+': 2,\n",
    "        '6B': 3,\n",
    "        '6B+': 4,\n",
    "        '6C': 5,\n",
    "        '6C+': 6,\n",
    "        '7A': 7,\n",
    "        '7A+': 8,\n",
    "        '7B': 9,\n",
    "        '7B+': 10,\n",
    "        '7C': 11,\n",
    "        '7C+': 12,\n",
    "        '8A': 13,\n",
    "        '8A+': 14,\n",
    "        '8B': 15,\n",
    "        '8B+': 16,\n",
    "        '8C': 17,\n",
    "        '8C+': 18\n",
    "    }\n",
    "\n",
    "grades = [grade.split()[0] for grade in grades]\n",
    "grades = [grade_map[grade] for grade in grades]\n",
    "len(grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProblemDataset(Dataset):\n",
    "    def __init__(self, data, label, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.label = label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        image_tensor = torch.from_numpy(image)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        label = self.label[idx]\n",
    "        \n",
    "        \n",
    "        target = torch.tensor(label,dtype=torch.long)\n",
    "        \n",
    "        return image_tensor, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(problems, grades, test_size=0.2, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ProblemDataset(X_train, y_train)\n",
    "val = ProblemDataset(X_val, y_val)\n",
    "test = ProblemDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=16)\n",
    "val_loader = DataLoader(val, batch_size=16)\n",
    "test_loader = DataLoader(test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "\n",
    "CUDA = True\n",
    "SEED = 1\n",
    "BATCH_SIZE = 16\n",
    "LOG_INTERVAL = 10\n",
    "EPOCHS = 10\n",
    "no_of_sample = 10\n",
    "\n",
    "# connections through the autoencoder bottleneck\n",
    "# in the pytorch VAE example, this is 20\n",
    "ZDIMS = 20\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(4, 4), padding=(10, 7),\n",
    "                               stride=2)  # This padding keeps the size of the image same, i.e. same padding\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(4, 4), padding=(10, 7), stride=2)\n",
    "        self.fc11 = nn.Linear(in_features=128 * 18 * 11, out_features=1024)\n",
    "        self.fc12 = nn.Linear(in_features=1024, out_features=ZDIMS)\n",
    "\n",
    "        self.fc21 = nn.Linear(in_features=128 * 18 * 11, out_features=1024)\n",
    "        self.fc22 = nn.Linear(in_features=1024, out_features=ZDIMS)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # For decoder\n",
    "\n",
    "        # For mu\n",
    "        self.fc1 = nn.Linear(in_features=20, out_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=7 * 7 * 128)\n",
    "        self.conv_t1 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, padding=1, stride=2)\n",
    "        self.conv_t2 = nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=(7,2), padding=(1,2), stride=1)\n",
    "\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "\n",
    "        x = x.view(-1, 1, 18, 11)\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = x.view(-1, 128 * 18 * 11)\n",
    "\n",
    "        mu_z = F.elu(self.fc11(x))\n",
    "        mu_z = self.fc12(mu_z)\n",
    "\n",
    "        logvar_z = F.elu(self.fc21(x))\n",
    "        logvar_z = self.fc22(logvar_z)\n",
    "\n",
    "        return mu_z, logvar_z\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "\n",
    "\n",
    "        if self.training:\n",
    "            # multiply log variance with 0.5, then in-place exponent\n",
    "            # yielding the standard deviation\n",
    "\n",
    "            sample_z = []\n",
    "            for _ in range(no_of_sample):\n",
    "                std = logvar.mul(0.5).exp_()  \n",
    "                eps = Variable(std.data.new(std.size()).normal_())\n",
    "                sample_z.append(eps.mul(std).add_(mu))\n",
    "\n",
    "            return sample_z\n",
    "\n",
    "        else:\n",
    "            # During inference, we simply return the mean of the\n",
    "            # learned distribution for the current input.\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "\n",
    "        x = F.elu(self.fc1(z))\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = x.view(-1, 128, 7, 7)\n",
    "        x = F.relu(self.conv_t1(x))\n",
    "        x = F.sigmoid(self.conv_t2(x))\n",
    "        return x.view(-1, 198)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 198))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        if self.training:\n",
    "            return [self.decode(z) for z in z], mu, logvar\n",
    "        else:\n",
    "            return self.decode(z), mu, logvar\n",
    "        # return self.decode(z), mu, logvar\n",
    "\n",
    "    def loss_function(self, recon_x, x, mu, logvar):\n",
    "        # how well do input x and output recon_x agree?\n",
    "\n",
    "        if self.training:\n",
    "            BCE = 0\n",
    "            for recon_x_one in recon_x:\n",
    "                BCE += F.binary_cross_entropy(recon_x_one, x.view(-1, 198))\n",
    "            BCE /= len(recon_x)\n",
    "        else:\n",
    "            BCE = F.binary_cross_entropy(recon_x, x.view(-1, 198))\n",
    "\n",
    "        # KLD is Kullbackâ€“Leibler divergence -- how much does one learned\n",
    "        # distribution deviate from another, in this specific case the\n",
    "        # learned distribution from the unit Gaussian\n",
    "\n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # https://arxiv.org/abs/1312.6114\n",
    "        # - D_{KL} = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        # note the negative D_{KL} in appendix B of the paper\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        # Normalise by same number of elements as in reconstruction\n",
    "        KLD /= BATCH_SIZE * 198\n",
    "\n",
    "\n",
    "        return BCE + KLD\n",
    "\n",
    "\n",
    "model = VAE()\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    # toggle model to train mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        \n",
    "        if CUDA:\n",
    "            data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # push whole batch of data through VAE.forward() to get recon_loss\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        # calculate scalar loss\n",
    "        loss = model.loss_function(recon_batch, data, mu, logvar)\n",
    "        # calculate the gradient of the loss w.r.t. the graph leaves\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader),\n",
    "                       loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "\n",
    "    # each data is of BATCH_SIZE (default 128) samples\n",
    "    for i, (data, _) in enumerate(test_loader):\n",
    "        if CUDA:\n",
    "            # make sure this lives on the GPU\n",
    "            data = data.cuda()\n",
    "\n",
    "        # we're only going to infer, so no autograd at all required: volatile=True\n",
    "        data = Variable(data, volatile=True)\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        test_loss += model.loss_function(recon_batch, data, mu, logvar).item()\n",
    "        if i == 0:\n",
    "            n = min(data.size(0), 8)\n",
    "            # for the first 128 batch of the epoch, show the first 8 input problems\n",
    "            # and the reconstructions below them\n",
    "            comparison = torch.cat([data[:n],\n",
    "                                    recon_batch.view(BATCH_SIZE, 1, 18, 11)[:n]])\n",
    "            save_image(comparison.data.cpu(),\n",
    "                       'reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8940 (0%)]\tLoss: 0.054762\n",
      "Train Epoch: 1 [160/8940 (2%)]\tLoss: 0.022958\n",
      "Train Epoch: 1 [320/8940 (4%)]\tLoss: 0.013715\n",
      "Train Epoch: 1 [480/8940 (5%)]\tLoss: 0.011717\n",
      "Train Epoch: 1 [640/8940 (7%)]\tLoss: 0.011218\n",
      "Train Epoch: 1 [800/8940 (9%)]\tLoss: 0.011800\n",
      "Train Epoch: 1 [960/8940 (11%)]\tLoss: 0.011654\n",
      "Train Epoch: 1 [1120/8940 (13%)]\tLoss: 0.011507\n",
      "Train Epoch: 1 [1280/8940 (14%)]\tLoss: 0.012196\n",
      "Train Epoch: 1 [1440/8940 (16%)]\tLoss: 0.010592\n",
      "Train Epoch: 1 [1600/8940 (18%)]\tLoss: 0.010955\n",
      "Train Epoch: 1 [1760/8940 (20%)]\tLoss: 0.011309\n",
      "Train Epoch: 1 [1920/8940 (21%)]\tLoss: 0.010538\n",
      "Train Epoch: 1 [2080/8940 (23%)]\tLoss: 0.010573\n",
      "Train Epoch: 1 [2240/8940 (25%)]\tLoss: 0.011769\n",
      "Train Epoch: 1 [2400/8940 (27%)]\tLoss: 0.010376\n",
      "Train Epoch: 1 [2560/8940 (29%)]\tLoss: 0.010984\n",
      "Train Epoch: 1 [2720/8940 (30%)]\tLoss: 0.010437\n",
      "Train Epoch: 1 [2880/8940 (32%)]\tLoss: 0.010785\n",
      "Train Epoch: 1 [3040/8940 (34%)]\tLoss: 0.010771\n",
      "Train Epoch: 1 [3200/8940 (36%)]\tLoss: 0.010092\n",
      "Train Epoch: 1 [3360/8940 (38%)]\tLoss: 0.010420\n",
      "Train Epoch: 1 [3520/8940 (39%)]\tLoss: 0.009826\n",
      "Train Epoch: 1 [3680/8940 (41%)]\tLoss: 0.010347\n",
      "Train Epoch: 1 [3840/8940 (43%)]\tLoss: 0.010272\n",
      "Train Epoch: 1 [4000/8940 (45%)]\tLoss: 0.010945\n",
      "Train Epoch: 1 [4160/8940 (47%)]\tLoss: 0.009828\n",
      "Train Epoch: 1 [4320/8940 (48%)]\tLoss: 0.009137\n",
      "Train Epoch: 1 [4480/8940 (50%)]\tLoss: 0.010187\n",
      "Train Epoch: 1 [4640/8940 (52%)]\tLoss: 0.009939\n",
      "Train Epoch: 1 [4800/8940 (54%)]\tLoss: 0.010192\n",
      "Train Epoch: 1 [4960/8940 (55%)]\tLoss: 0.009847\n",
      "Train Epoch: 1 [5120/8940 (57%)]\tLoss: 0.008765\n",
      "Train Epoch: 1 [5280/8940 (59%)]\tLoss: 0.008840\n",
      "Train Epoch: 1 [5440/8940 (61%)]\tLoss: 0.009178\n",
      "Train Epoch: 1 [5600/8940 (63%)]\tLoss: 0.009731\n",
      "Train Epoch: 1 [5760/8940 (64%)]\tLoss: 0.009105\n",
      "Train Epoch: 1 [5920/8940 (66%)]\tLoss: 0.009510\n",
      "Train Epoch: 1 [6080/8940 (68%)]\tLoss: 0.009100\n",
      "Train Epoch: 1 [6240/8940 (70%)]\tLoss: 0.009466\n",
      "Train Epoch: 1 [6400/8940 (72%)]\tLoss: 0.010256\n",
      "Train Epoch: 1 [6560/8940 (73%)]\tLoss: 0.009109\n",
      "Train Epoch: 1 [6720/8940 (75%)]\tLoss: 0.009488\n",
      "Train Epoch: 1 [6880/8940 (77%)]\tLoss: 0.009102\n",
      "Train Epoch: 1 [7040/8940 (79%)]\tLoss: 0.008926\n",
      "Train Epoch: 1 [7200/8940 (81%)]\tLoss: 0.008536\n",
      "Train Epoch: 1 [7360/8940 (82%)]\tLoss: 0.008477\n",
      "Train Epoch: 1 [7520/8940 (84%)]\tLoss: 0.009186\n",
      "Train Epoch: 1 [7680/8940 (86%)]\tLoss: 0.009721\n",
      "Train Epoch: 1 [7840/8940 (88%)]\tLoss: 0.009028\n",
      "Train Epoch: 1 [8000/8940 (89%)]\tLoss: 0.009963\n",
      "Train Epoch: 1 [8160/8940 (91%)]\tLoss: 0.009672\n",
      "Train Epoch: 1 [8320/8940 (93%)]\tLoss: 0.009508\n",
      "Train Epoch: 1 [8480/8940 (95%)]\tLoss: 0.010689\n",
      "Train Epoch: 1 [8640/8940 (97%)]\tLoss: 0.009154\n",
      "Train Epoch: 1 [8800/8940 (98%)]\tLoss: 0.008912\n",
      "====> Epoch: 1 Average loss: 0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:193: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Test set loss: 0.0087\n",
      "Train Epoch: 2 [0/8940 (0%)]\tLoss: 0.009591\n",
      "Train Epoch: 2 [160/8940 (2%)]\tLoss: 0.009327\n",
      "Train Epoch: 2 [320/8940 (4%)]\tLoss: 0.009045\n",
      "Train Epoch: 2 [480/8940 (5%)]\tLoss: 0.008844\n",
      "Train Epoch: 2 [640/8940 (7%)]\tLoss: 0.008781\n",
      "Train Epoch: 2 [800/8940 (9%)]\tLoss: 0.009474\n",
      "Train Epoch: 2 [960/8940 (11%)]\tLoss: 0.009402\n",
      "Train Epoch: 2 [1120/8940 (13%)]\tLoss: 0.009644\n",
      "Train Epoch: 2 [1280/8940 (14%)]\tLoss: 0.010434\n",
      "Train Epoch: 2 [1440/8940 (16%)]\tLoss: 0.008938\n",
      "Train Epoch: 2 [1600/8940 (18%)]\tLoss: 0.009017\n",
      "Train Epoch: 2 [1760/8940 (20%)]\tLoss: 0.010044\n",
      "Train Epoch: 2 [1920/8940 (21%)]\tLoss: 0.008833\n",
      "Train Epoch: 2 [2080/8940 (23%)]\tLoss: 0.009146\n",
      "Train Epoch: 2 [2240/8940 (25%)]\tLoss: 0.009887\n",
      "Train Epoch: 2 [2400/8940 (27%)]\tLoss: 0.009261\n",
      "Train Epoch: 2 [2560/8940 (29%)]\tLoss: 0.009873\n",
      "Train Epoch: 2 [2720/8940 (30%)]\tLoss: 0.009431\n",
      "Train Epoch: 2 [2880/8940 (32%)]\tLoss: 0.009945\n",
      "Train Epoch: 2 [3040/8940 (34%)]\tLoss: 0.009487\n",
      "Train Epoch: 2 [3200/8940 (36%)]\tLoss: 0.009117\n",
      "Train Epoch: 2 [3360/8940 (38%)]\tLoss: 0.009250\n",
      "Train Epoch: 2 [3520/8940 (39%)]\tLoss: 0.008982\n",
      "Train Epoch: 2 [3680/8940 (41%)]\tLoss: 0.009255\n",
      "Train Epoch: 2 [3840/8940 (43%)]\tLoss: 0.009620\n",
      "Train Epoch: 2 [4000/8940 (45%)]\tLoss: 0.010044\n",
      "Train Epoch: 2 [4160/8940 (47%)]\tLoss: 0.008947\n",
      "Train Epoch: 2 [4320/8940 (48%)]\tLoss: 0.008542\n",
      "Train Epoch: 2 [4480/8940 (50%)]\tLoss: 0.009489\n",
      "Train Epoch: 2 [4640/8940 (52%)]\tLoss: 0.009601\n",
      "Train Epoch: 2 [4800/8940 (54%)]\tLoss: 0.009575\n",
      "Train Epoch: 2 [4960/8940 (55%)]\tLoss: 0.009226\n",
      "Train Epoch: 2 [5120/8940 (57%)]\tLoss: 0.008248\n",
      "Train Epoch: 2 [5280/8940 (59%)]\tLoss: 0.008455\n",
      "Train Epoch: 2 [5440/8940 (61%)]\tLoss: 0.008796\n",
      "Train Epoch: 2 [5600/8940 (63%)]\tLoss: 0.009203\n",
      "Train Epoch: 2 [5760/8940 (64%)]\tLoss: 0.008692\n",
      "Train Epoch: 2 [5920/8940 (66%)]\tLoss: 0.008998\n",
      "Train Epoch: 2 [6080/8940 (68%)]\tLoss: 0.008692\n",
      "Train Epoch: 2 [6240/8940 (70%)]\tLoss: 0.009018\n",
      "Train Epoch: 2 [6400/8940 (72%)]\tLoss: 0.009931\n",
      "Train Epoch: 2 [6560/8940 (73%)]\tLoss: 0.008753\n",
      "Train Epoch: 2 [6720/8940 (75%)]\tLoss: 0.008966\n",
      "Train Epoch: 2 [6880/8940 (77%)]\tLoss: 0.008664\n",
      "Train Epoch: 2 [7040/8940 (79%)]\tLoss: 0.008462\n",
      "Train Epoch: 2 [7200/8940 (81%)]\tLoss: 0.008383\n",
      "Train Epoch: 2 [7360/8940 (82%)]\tLoss: 0.008220\n",
      "Train Epoch: 2 [7520/8940 (84%)]\tLoss: 0.008800\n",
      "Train Epoch: 2 [7680/8940 (86%)]\tLoss: 0.009311\n",
      "Train Epoch: 2 [7840/8940 (88%)]\tLoss: 0.008834\n",
      "Train Epoch: 2 [8000/8940 (89%)]\tLoss: 0.009600\n",
      "Train Epoch: 2 [8160/8940 (91%)]\tLoss: 0.009316\n",
      "Train Epoch: 2 [8320/8940 (93%)]\tLoss: 0.009345\n",
      "Train Epoch: 2 [8480/8940 (95%)]\tLoss: 0.010381\n",
      "Train Epoch: 2 [8640/8940 (97%)]\tLoss: 0.008885\n",
      "Train Epoch: 2 [8800/8940 (98%)]\tLoss: 0.008638\n",
      "====> Epoch: 2 Average loss: 0.0092\n",
      "====> Test set loss: 0.0082\n",
      "Train Epoch: 3 [0/8940 (0%)]\tLoss: 0.009282\n",
      "Train Epoch: 3 [160/8940 (2%)]\tLoss: 0.008914\n",
      "Train Epoch: 3 [320/8940 (4%)]\tLoss: 0.008902\n",
      "Train Epoch: 3 [480/8940 (5%)]\tLoss: 0.008575\n",
      "Train Epoch: 3 [640/8940 (7%)]\tLoss: 0.008531\n",
      "Train Epoch: 3 [800/8940 (9%)]\tLoss: 0.009249\n",
      "Train Epoch: 3 [960/8940 (11%)]\tLoss: 0.009174\n",
      "Train Epoch: 3 [1120/8940 (13%)]\tLoss: 0.009229\n",
      "Train Epoch: 3 [1280/8940 (14%)]\tLoss: 0.010410\n",
      "Train Epoch: 3 [1440/8940 (16%)]\tLoss: 0.008844\n",
      "Train Epoch: 3 [1600/8940 (18%)]\tLoss: 0.008802\n",
      "Train Epoch: 3 [1760/8940 (20%)]\tLoss: 0.009938\n",
      "Train Epoch: 3 [1920/8940 (21%)]\tLoss: 0.008562\n",
      "Train Epoch: 3 [2080/8940 (23%)]\tLoss: 0.008919\n",
      "Train Epoch: 3 [2240/8940 (25%)]\tLoss: 0.009744\n",
      "Train Epoch: 3 [2400/8940 (27%)]\tLoss: 0.009048\n",
      "Train Epoch: 3 [2560/8940 (29%)]\tLoss: 0.009717\n",
      "Train Epoch: 3 [2720/8940 (30%)]\tLoss: 0.009240\n",
      "Train Epoch: 3 [2880/8940 (32%)]\tLoss: 0.009899\n",
      "Train Epoch: 3 [3040/8940 (34%)]\tLoss: 0.009340\n",
      "Train Epoch: 3 [3200/8940 (36%)]\tLoss: 0.008905\n",
      "Train Epoch: 3 [3360/8940 (38%)]\tLoss: 0.009148\n",
      "Train Epoch: 3 [3520/8940 (39%)]\tLoss: 0.008876\n",
      "Train Epoch: 3 [3680/8940 (41%)]\tLoss: 0.009083\n",
      "Train Epoch: 3 [3840/8940 (43%)]\tLoss: 0.009580\n",
      "Train Epoch: 3 [4000/8940 (45%)]\tLoss: 0.009791\n",
      "Train Epoch: 3 [4160/8940 (47%)]\tLoss: 0.008678\n",
      "Train Epoch: 3 [4320/8940 (48%)]\tLoss: 0.008438\n",
      "Train Epoch: 3 [4480/8940 (50%)]\tLoss: 0.009444\n",
      "Train Epoch: 3 [4640/8940 (52%)]\tLoss: 0.009542\n",
      "Train Epoch: 3 [4800/8940 (54%)]\tLoss: 0.009353\n",
      "Train Epoch: 3 [4960/8940 (55%)]\tLoss: 0.009005\n",
      "Train Epoch: 3 [5120/8940 (57%)]\tLoss: 0.008242\n",
      "Train Epoch: 3 [5280/8940 (59%)]\tLoss: 0.008285\n",
      "Train Epoch: 3 [5440/8940 (61%)]\tLoss: 0.008667\n",
      "Train Epoch: 3 [5600/8940 (63%)]\tLoss: 0.009074\n",
      "Train Epoch: 3 [5760/8940 (64%)]\tLoss: 0.008559\n",
      "Train Epoch: 3 [5920/8940 (66%)]\tLoss: 0.008939\n",
      "Train Epoch: 3 [6080/8940 (68%)]\tLoss: 0.008466\n",
      "Train Epoch: 3 [6240/8940 (70%)]\tLoss: 0.008911\n",
      "Train Epoch: 3 [6400/8940 (72%)]\tLoss: 0.009927\n",
      "Train Epoch: 3 [6560/8940 (73%)]\tLoss: 0.008475\n",
      "Train Epoch: 3 [6720/8940 (75%)]\tLoss: 0.008906\n",
      "Train Epoch: 3 [6880/8940 (77%)]\tLoss: 0.008467\n",
      "Train Epoch: 3 [7040/8940 (79%)]\tLoss: 0.008332\n",
      "Train Epoch: 3 [7200/8940 (81%)]\tLoss: 0.008425\n",
      "Train Epoch: 3 [7360/8940 (82%)]\tLoss: 0.008133\n",
      "Train Epoch: 3 [7520/8940 (84%)]\tLoss: 0.008630\n",
      "Train Epoch: 3 [7680/8940 (86%)]\tLoss: 0.009379\n",
      "Train Epoch: 3 [7840/8940 (88%)]\tLoss: 0.008889\n",
      "Train Epoch: 3 [8000/8940 (89%)]\tLoss: 0.009541\n",
      "Train Epoch: 3 [8160/8940 (91%)]\tLoss: 0.009269\n",
      "Train Epoch: 3 [8320/8940 (93%)]\tLoss: 0.009246\n",
      "Train Epoch: 3 [8480/8940 (95%)]\tLoss: 0.010270\n",
      "Train Epoch: 3 [8640/8940 (97%)]\tLoss: 0.008946\n",
      "Train Epoch: 3 [8800/8940 (98%)]\tLoss: 0.008379\n",
      "====> Epoch: 3 Average loss: 0.0091\n",
      "====> Test set loss: 0.0080\n",
      "Train Epoch: 4 [0/8940 (0%)]\tLoss: 0.009111\n",
      "Train Epoch: 4 [160/8940 (2%)]\tLoss: 0.008800\n",
      "Train Epoch: 4 [320/8940 (4%)]\tLoss: 0.008815\n",
      "Train Epoch: 4 [480/8940 (5%)]\tLoss: 0.008572\n",
      "Train Epoch: 4 [640/8940 (7%)]\tLoss: 0.008446\n",
      "Train Epoch: 4 [800/8940 (9%)]\tLoss: 0.009225\n",
      "Train Epoch: 4 [960/8940 (11%)]\tLoss: 0.009113\n",
      "Train Epoch: 4 [1120/8940 (13%)]\tLoss: 0.009121\n",
      "Train Epoch: 4 [1280/8940 (14%)]\tLoss: 0.010374\n",
      "Train Epoch: 4 [1440/8940 (16%)]\tLoss: 0.008673\n",
      "Train Epoch: 4 [1600/8940 (18%)]\tLoss: 0.008691\n",
      "Train Epoch: 4 [1760/8940 (20%)]\tLoss: 0.009955\n",
      "Train Epoch: 4 [1920/8940 (21%)]\tLoss: 0.008501\n",
      "Train Epoch: 4 [2080/8940 (23%)]\tLoss: 0.008790\n",
      "Train Epoch: 4 [2240/8940 (25%)]\tLoss: 0.009570\n",
      "Train Epoch: 4 [2400/8940 (27%)]\tLoss: 0.009059\n",
      "Train Epoch: 4 [2560/8940 (29%)]\tLoss: 0.009656\n",
      "Train Epoch: 4 [2720/8940 (30%)]\tLoss: 0.009232\n",
      "Train Epoch: 4 [2880/8940 (32%)]\tLoss: 0.009907\n",
      "Train Epoch: 4 [3040/8940 (34%)]\tLoss: 0.009177\n",
      "Train Epoch: 4 [3200/8940 (36%)]\tLoss: 0.008802\n",
      "Train Epoch: 4 [3360/8940 (38%)]\tLoss: 0.008996\n",
      "Train Epoch: 4 [3520/8940 (39%)]\tLoss: 0.008817\n",
      "Train Epoch: 4 [3680/8940 (41%)]\tLoss: 0.008961\n",
      "Train Epoch: 4 [3840/8940 (43%)]\tLoss: 0.009368\n",
      "Train Epoch: 4 [4000/8940 (45%)]\tLoss: 0.009756\n",
      "Train Epoch: 4 [4160/8940 (47%)]\tLoss: 0.008609\n",
      "Train Epoch: 4 [4320/8940 (48%)]\tLoss: 0.008337\n",
      "Train Epoch: 4 [4480/8940 (50%)]\tLoss: 0.009353\n",
      "Train Epoch: 4 [4640/8940 (52%)]\tLoss: 0.009361\n",
      "Train Epoch: 4 [4800/8940 (54%)]\tLoss: 0.009140\n",
      "Train Epoch: 4 [4960/8940 (55%)]\tLoss: 0.009076\n",
      "Train Epoch: 4 [5120/8940 (57%)]\tLoss: 0.008118\n",
      "Train Epoch: 4 [5280/8940 (59%)]\tLoss: 0.008307\n",
      "Train Epoch: 4 [5440/8940 (61%)]\tLoss: 0.008545\n",
      "Train Epoch: 4 [5600/8940 (63%)]\tLoss: 0.008933\n",
      "Train Epoch: 4 [5760/8940 (64%)]\tLoss: 0.008468\n",
      "Train Epoch: 4 [5920/8940 (66%)]\tLoss: 0.008846\n",
      "Train Epoch: 4 [6080/8940 (68%)]\tLoss: 0.008477\n",
      "Train Epoch: 4 [6240/8940 (70%)]\tLoss: 0.008908\n",
      "Train Epoch: 4 [6400/8940 (72%)]\tLoss: 0.009928\n",
      "Train Epoch: 4 [6560/8940 (73%)]\tLoss: 0.008560\n",
      "Train Epoch: 4 [6720/8940 (75%)]\tLoss: 0.008870\n",
      "Train Epoch: 4 [6880/8940 (77%)]\tLoss: 0.008342\n",
      "Train Epoch: 4 [7040/8940 (79%)]\tLoss: 0.008412\n",
      "Train Epoch: 4 [7200/8940 (81%)]\tLoss: 0.008339\n",
      "Train Epoch: 4 [7360/8940 (82%)]\tLoss: 0.008074\n",
      "Train Epoch: 4 [7520/8940 (84%)]\tLoss: 0.008609\n",
      "Train Epoch: 4 [7680/8940 (86%)]\tLoss: 0.009284\n",
      "Train Epoch: 4 [7840/8940 (88%)]\tLoss: 0.008727\n",
      "Train Epoch: 4 [8000/8940 (89%)]\tLoss: 0.009565\n",
      "Train Epoch: 4 [8160/8940 (91%)]\tLoss: 0.009204\n",
      "Train Epoch: 4 [8320/8940 (93%)]\tLoss: 0.009322\n",
      "Train Epoch: 4 [8480/8940 (95%)]\tLoss: 0.010070\n",
      "Train Epoch: 4 [8640/8940 (97%)]\tLoss: 0.008775\n",
      "Train Epoch: 4 [8800/8940 (98%)]\tLoss: 0.008511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 4 Average loss: 0.0090\n",
      "====> Test set loss: 0.0077\n",
      "Train Epoch: 5 [0/8940 (0%)]\tLoss: 0.009097\n",
      "Train Epoch: 5 [160/8940 (2%)]\tLoss: 0.008778\n",
      "Train Epoch: 5 [320/8940 (4%)]\tLoss: 0.008715\n",
      "Train Epoch: 5 [480/8940 (5%)]\tLoss: 0.008434\n",
      "Train Epoch: 5 [640/8940 (7%)]\tLoss: 0.008285\n",
      "Train Epoch: 5 [800/8940 (9%)]\tLoss: 0.009046\n",
      "Train Epoch: 5 [960/8940 (11%)]\tLoss: 0.008919\n",
      "Train Epoch: 5 [1120/8940 (13%)]\tLoss: 0.008986\n",
      "Train Epoch: 5 [1280/8940 (14%)]\tLoss: 0.010309\n",
      "Train Epoch: 5 [1440/8940 (16%)]\tLoss: 0.008752\n",
      "Train Epoch: 5 [1600/8940 (18%)]\tLoss: 0.008735\n",
      "Train Epoch: 5 [1760/8940 (20%)]\tLoss: 0.009884\n",
      "Train Epoch: 5 [1920/8940 (21%)]\tLoss: 0.008299\n",
      "Train Epoch: 5 [2080/8940 (23%)]\tLoss: 0.008691\n",
      "Train Epoch: 5 [2240/8940 (25%)]\tLoss: 0.009549\n",
      "Train Epoch: 5 [2400/8940 (27%)]\tLoss: 0.008933\n",
      "Train Epoch: 5 [2560/8940 (29%)]\tLoss: 0.009597\n",
      "Train Epoch: 5 [2720/8940 (30%)]\tLoss: 0.009078\n",
      "Train Epoch: 5 [2880/8940 (32%)]\tLoss: 0.009836\n",
      "Train Epoch: 5 [3040/8940 (34%)]\tLoss: 0.009178\n",
      "Train Epoch: 5 [3200/8940 (36%)]\tLoss: 0.008802\n",
      "Train Epoch: 5 [3360/8940 (38%)]\tLoss: 0.008902\n",
      "Train Epoch: 5 [3520/8940 (39%)]\tLoss: 0.008626\n",
      "Train Epoch: 5 [3680/8940 (41%)]\tLoss: 0.009043\n",
      "Train Epoch: 5 [3840/8940 (43%)]\tLoss: 0.009292\n",
      "Train Epoch: 5 [4000/8940 (45%)]\tLoss: 0.009681\n",
      "Train Epoch: 5 [4160/8940 (47%)]\tLoss: 0.008632\n",
      "Train Epoch: 5 [4320/8940 (48%)]\tLoss: 0.008408\n",
      "Train Epoch: 5 [4480/8940 (50%)]\tLoss: 0.009362\n",
      "Train Epoch: 5 [4640/8940 (52%)]\tLoss: 0.009319\n",
      "Train Epoch: 5 [4800/8940 (54%)]\tLoss: 0.009024\n",
      "Train Epoch: 5 [4960/8940 (55%)]\tLoss: 0.008936\n",
      "Train Epoch: 5 [5120/8940 (57%)]\tLoss: 0.007965\n",
      "Train Epoch: 5 [5280/8940 (59%)]\tLoss: 0.008210\n",
      "Train Epoch: 5 [5440/8940 (61%)]\tLoss: 0.008692\n",
      "Train Epoch: 5 [5600/8940 (63%)]\tLoss: 0.008884\n",
      "Train Epoch: 5 [5760/8940 (64%)]\tLoss: 0.008516\n",
      "Train Epoch: 5 [5920/8940 (66%)]\tLoss: 0.008868\n",
      "Train Epoch: 5 [6080/8940 (68%)]\tLoss: 0.008450\n",
      "Train Epoch: 5 [6240/8940 (70%)]\tLoss: 0.008741\n",
      "Train Epoch: 5 [6400/8940 (72%)]\tLoss: 0.009905\n",
      "Train Epoch: 5 [6560/8940 (73%)]\tLoss: 0.008349\n",
      "Train Epoch: 5 [6720/8940 (75%)]\tLoss: 0.008789\n",
      "Train Epoch: 5 [6880/8940 (77%)]\tLoss: 0.008263\n",
      "Train Epoch: 5 [7040/8940 (79%)]\tLoss: 0.008199\n",
      "Train Epoch: 5 [7200/8940 (81%)]\tLoss: 0.008201\n",
      "Train Epoch: 5 [7360/8940 (82%)]\tLoss: 0.007868\n",
      "Train Epoch: 5 [7520/8940 (84%)]\tLoss: 0.008521\n",
      "Train Epoch: 5 [7680/8940 (86%)]\tLoss: 0.009261\n",
      "Train Epoch: 5 [7840/8940 (88%)]\tLoss: 0.008674\n",
      "Train Epoch: 5 [8000/8940 (89%)]\tLoss: 0.009466\n",
      "Train Epoch: 5 [8160/8940 (91%)]\tLoss: 0.009100\n",
      "Train Epoch: 5 [8320/8940 (93%)]\tLoss: 0.009041\n",
      "Train Epoch: 5 [8480/8940 (95%)]\tLoss: 0.009897\n",
      "Train Epoch: 5 [8640/8940 (97%)]\tLoss: 0.008819\n",
      "Train Epoch: 5 [8800/8940 (98%)]\tLoss: 0.008358\n",
      "====> Epoch: 5 Average loss: 0.0089\n",
      "====> Test set loss: 0.0076\n",
      "Train Epoch: 6 [0/8940 (0%)]\tLoss: 0.009132\n",
      "Train Epoch: 6 [160/8940 (2%)]\tLoss: 0.008769\n",
      "Train Epoch: 6 [320/8940 (4%)]\tLoss: 0.008620\n",
      "Train Epoch: 6 [480/8940 (5%)]\tLoss: 0.008300\n",
      "Train Epoch: 6 [640/8940 (7%)]\tLoss: 0.008275\n",
      "Train Epoch: 6 [800/8940 (9%)]\tLoss: 0.008978\n",
      "Train Epoch: 6 [960/8940 (11%)]\tLoss: 0.008885\n",
      "Train Epoch: 6 [1120/8940 (13%)]\tLoss: 0.008973\n",
      "Train Epoch: 6 [1280/8940 (14%)]\tLoss: 0.010215\n",
      "Train Epoch: 6 [1440/8940 (16%)]\tLoss: 0.008722\n",
      "Train Epoch: 6 [1600/8940 (18%)]\tLoss: 0.008708\n",
      "Train Epoch: 6 [1760/8940 (20%)]\tLoss: 0.009864\n",
      "Train Epoch: 6 [1920/8940 (21%)]\tLoss: 0.008378\n",
      "Train Epoch: 6 [2080/8940 (23%)]\tLoss: 0.008715\n",
      "Train Epoch: 6 [2240/8940 (25%)]\tLoss: 0.009449\n",
      "Train Epoch: 6 [2400/8940 (27%)]\tLoss: 0.008957\n",
      "Train Epoch: 6 [2560/8940 (29%)]\tLoss: 0.009430\n",
      "Train Epoch: 6 [2720/8940 (30%)]\tLoss: 0.009102\n",
      "Train Epoch: 6 [2880/8940 (32%)]\tLoss: 0.009787\n",
      "Train Epoch: 6 [3040/8940 (34%)]\tLoss: 0.009067\n",
      "Train Epoch: 6 [3200/8940 (36%)]\tLoss: 0.008782\n",
      "Train Epoch: 6 [3360/8940 (38%)]\tLoss: 0.008872\n",
      "Train Epoch: 6 [3520/8940 (39%)]\tLoss: 0.008624\n",
      "Train Epoch: 6 [3680/8940 (41%)]\tLoss: 0.008832\n",
      "Train Epoch: 6 [3840/8940 (43%)]\tLoss: 0.009220\n",
      "Train Epoch: 6 [4000/8940 (45%)]\tLoss: 0.009517\n",
      "Train Epoch: 6 [4160/8940 (47%)]\tLoss: 0.008538\n",
      "Train Epoch: 6 [4320/8940 (48%)]\tLoss: 0.008280\n",
      "Train Epoch: 6 [4480/8940 (50%)]\tLoss: 0.009271\n",
      "Train Epoch: 6 [4640/8940 (52%)]\tLoss: 0.009268\n",
      "Train Epoch: 6 [4800/8940 (54%)]\tLoss: 0.009047\n",
      "Train Epoch: 6 [4960/8940 (55%)]\tLoss: 0.008849\n",
      "Train Epoch: 6 [5120/8940 (57%)]\tLoss: 0.007970\n",
      "Train Epoch: 6 [5280/8940 (59%)]\tLoss: 0.008167\n",
      "Train Epoch: 6 [5440/8940 (61%)]\tLoss: 0.008486\n",
      "Train Epoch: 6 [5600/8940 (63%)]\tLoss: 0.008922\n",
      "Train Epoch: 6 [5760/8940 (64%)]\tLoss: 0.008349\n",
      "Train Epoch: 6 [5920/8940 (66%)]\tLoss: 0.008744\n",
      "Train Epoch: 6 [6080/8940 (68%)]\tLoss: 0.008265\n",
      "Train Epoch: 6 [6240/8940 (70%)]\tLoss: 0.008856\n",
      "Train Epoch: 6 [6400/8940 (72%)]\tLoss: 0.009785\n",
      "Train Epoch: 6 [6560/8940 (73%)]\tLoss: 0.008275\n",
      "Train Epoch: 6 [6720/8940 (75%)]\tLoss: 0.008787\n",
      "Train Epoch: 6 [6880/8940 (77%)]\tLoss: 0.008181\n",
      "Train Epoch: 6 [7040/8940 (79%)]\tLoss: 0.008358\n",
      "Train Epoch: 6 [7200/8940 (81%)]\tLoss: 0.008349\n",
      "Train Epoch: 6 [7360/8940 (82%)]\tLoss: 0.007983\n",
      "Train Epoch: 6 [7520/8940 (84%)]\tLoss: 0.008679\n",
      "Train Epoch: 6 [7680/8940 (86%)]\tLoss: 0.009180\n",
      "Train Epoch: 6 [7840/8940 (88%)]\tLoss: 0.008749\n",
      "Train Epoch: 6 [8000/8940 (89%)]\tLoss: 0.009318\n",
      "Train Epoch: 6 [8160/8940 (91%)]\tLoss: 0.008880\n",
      "Train Epoch: 6 [8320/8940 (93%)]\tLoss: 0.009005\n",
      "Train Epoch: 6 [8480/8940 (95%)]\tLoss: 0.009826\n",
      "Train Epoch: 6 [8640/8940 (97%)]\tLoss: 0.008672\n",
      "Train Epoch: 6 [8800/8940 (98%)]\tLoss: 0.008292\n",
      "====> Epoch: 6 Average loss: 0.0089\n",
      "====> Test set loss: 0.0076\n",
      "Train Epoch: 7 [0/8940 (0%)]\tLoss: 0.008982\n",
      "Train Epoch: 7 [160/8940 (2%)]\tLoss: 0.008845\n",
      "Train Epoch: 7 [320/8940 (4%)]\tLoss: 0.008444\n",
      "Train Epoch: 7 [480/8940 (5%)]\tLoss: 0.008204\n",
      "Train Epoch: 7 [640/8940 (7%)]\tLoss: 0.008359\n",
      "Train Epoch: 7 [800/8940 (9%)]\tLoss: 0.008990\n",
      "Train Epoch: 7 [960/8940 (11%)]\tLoss: 0.008856\n",
      "Train Epoch: 7 [1120/8940 (13%)]\tLoss: 0.008901\n",
      "Train Epoch: 7 [1280/8940 (14%)]\tLoss: 0.010214\n",
      "Train Epoch: 7 [1440/8940 (16%)]\tLoss: 0.008754\n",
      "Train Epoch: 7 [1600/8940 (18%)]\tLoss: 0.008726\n",
      "Train Epoch: 7 [1760/8940 (20%)]\tLoss: 0.009848\n",
      "Train Epoch: 7 [1920/8940 (21%)]\tLoss: 0.008417\n",
      "Train Epoch: 7 [2080/8940 (23%)]\tLoss: 0.008516\n",
      "Train Epoch: 7 [2240/8940 (25%)]\tLoss: 0.009352\n",
      "Train Epoch: 7 [2400/8940 (27%)]\tLoss: 0.008828\n",
      "Train Epoch: 7 [2560/8940 (29%)]\tLoss: 0.009416\n",
      "Train Epoch: 7 [2720/8940 (30%)]\tLoss: 0.009157\n",
      "Train Epoch: 7 [2880/8940 (32%)]\tLoss: 0.009684\n",
      "Train Epoch: 7 [3040/8940 (34%)]\tLoss: 0.009091\n",
      "Train Epoch: 7 [3200/8940 (36%)]\tLoss: 0.008694\n",
      "Train Epoch: 7 [3360/8940 (38%)]\tLoss: 0.008860\n",
      "Train Epoch: 7 [3520/8940 (39%)]\tLoss: 0.008617\n",
      "Train Epoch: 7 [3680/8940 (41%)]\tLoss: 0.008834\n",
      "Train Epoch: 7 [3840/8940 (43%)]\tLoss: 0.009155\n",
      "Train Epoch: 7 [4000/8940 (45%)]\tLoss: 0.009561\n",
      "Train Epoch: 7 [4160/8940 (47%)]\tLoss: 0.008689\n",
      "Train Epoch: 7 [4320/8940 (48%)]\tLoss: 0.008298\n",
      "Train Epoch: 7 [4480/8940 (50%)]\tLoss: 0.009136\n",
      "Train Epoch: 7 [4640/8940 (52%)]\tLoss: 0.009109\n",
      "Train Epoch: 7 [4800/8940 (54%)]\tLoss: 0.009033\n",
      "Train Epoch: 7 [4960/8940 (55%)]\tLoss: 0.008774\n",
      "Train Epoch: 7 [5120/8940 (57%)]\tLoss: 0.008055\n",
      "Train Epoch: 7 [5280/8940 (59%)]\tLoss: 0.008230\n",
      "Train Epoch: 7 [5440/8940 (61%)]\tLoss: 0.008475\n",
      "Train Epoch: 7 [5600/8940 (63%)]\tLoss: 0.008962\n",
      "Train Epoch: 7 [5760/8940 (64%)]\tLoss: 0.008306\n",
      "Train Epoch: 7 [5920/8940 (66%)]\tLoss: 0.008598\n",
      "Train Epoch: 7 [6080/8940 (68%)]\tLoss: 0.008309\n",
      "Train Epoch: 7 [6240/8940 (70%)]\tLoss: 0.008815\n",
      "Train Epoch: 7 [6400/8940 (72%)]\tLoss: 0.009736\n",
      "Train Epoch: 7 [6560/8940 (73%)]\tLoss: 0.008458\n",
      "Train Epoch: 7 [6720/8940 (75%)]\tLoss: 0.008750\n",
      "Train Epoch: 7 [6880/8940 (77%)]\tLoss: 0.008222\n",
      "Train Epoch: 7 [7040/8940 (79%)]\tLoss: 0.008127\n",
      "Train Epoch: 7 [7200/8940 (81%)]\tLoss: 0.008148\n",
      "Train Epoch: 7 [7360/8940 (82%)]\tLoss: 0.007914\n",
      "Train Epoch: 7 [7520/8940 (84%)]\tLoss: 0.008491\n",
      "Train Epoch: 7 [7680/8940 (86%)]\tLoss: 0.009250\n",
      "Train Epoch: 7 [7840/8940 (88%)]\tLoss: 0.008671\n",
      "Train Epoch: 7 [8000/8940 (89%)]\tLoss: 0.009243\n",
      "Train Epoch: 7 [8160/8940 (91%)]\tLoss: 0.008820\n",
      "Train Epoch: 7 [8320/8940 (93%)]\tLoss: 0.009050\n",
      "Train Epoch: 7 [8480/8940 (95%)]\tLoss: 0.009832\n",
      "Train Epoch: 7 [8640/8940 (97%)]\tLoss: 0.008674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [8800/8940 (98%)]\tLoss: 0.008300\n",
      "====> Epoch: 7 Average loss: 0.0089\n",
      "====> Test set loss: 0.0075\n",
      "Train Epoch: 8 [0/8940 (0%)]\tLoss: 0.008983\n",
      "Train Epoch: 8 [160/8940 (2%)]\tLoss: 0.008540\n",
      "Train Epoch: 8 [320/8940 (4%)]\tLoss: 0.008502\n",
      "Train Epoch: 8 [480/8940 (5%)]\tLoss: 0.008160\n",
      "Train Epoch: 8 [640/8940 (7%)]\tLoss: 0.008252\n",
      "Train Epoch: 8 [800/8940 (9%)]\tLoss: 0.009051\n",
      "Train Epoch: 8 [960/8940 (11%)]\tLoss: 0.008774\n",
      "Train Epoch: 8 [1120/8940 (13%)]\tLoss: 0.008737\n",
      "Train Epoch: 8 [1280/8940 (14%)]\tLoss: 0.010111\n",
      "Train Epoch: 8 [1440/8940 (16%)]\tLoss: 0.008535\n",
      "Train Epoch: 8 [1600/8940 (18%)]\tLoss: 0.008488\n",
      "Train Epoch: 8 [1760/8940 (20%)]\tLoss: 0.009784\n",
      "Train Epoch: 8 [1920/8940 (21%)]\tLoss: 0.008255\n",
      "Train Epoch: 8 [2080/8940 (23%)]\tLoss: 0.008663\n",
      "Train Epoch: 8 [2240/8940 (25%)]\tLoss: 0.009450\n",
      "Train Epoch: 8 [2400/8940 (27%)]\tLoss: 0.008957\n",
      "Train Epoch: 8 [2560/8940 (29%)]\tLoss: 0.009420\n",
      "Train Epoch: 8 [2720/8940 (30%)]\tLoss: 0.009002\n",
      "Train Epoch: 8 [2880/8940 (32%)]\tLoss: 0.009685\n",
      "Train Epoch: 8 [3040/8940 (34%)]\tLoss: 0.008962\n",
      "Train Epoch: 8 [3200/8940 (36%)]\tLoss: 0.008690\n",
      "Train Epoch: 8 [3360/8940 (38%)]\tLoss: 0.008834\n",
      "Train Epoch: 8 [3520/8940 (39%)]\tLoss: 0.008463\n",
      "Train Epoch: 8 [3680/8940 (41%)]\tLoss: 0.008806\n",
      "Train Epoch: 8 [3840/8940 (43%)]\tLoss: 0.008984\n",
      "Train Epoch: 8 [4000/8940 (45%)]\tLoss: 0.009435\n",
      "Train Epoch: 8 [4160/8940 (47%)]\tLoss: 0.008625\n",
      "Train Epoch: 8 [4320/8940 (48%)]\tLoss: 0.008214\n",
      "Train Epoch: 8 [4480/8940 (50%)]\tLoss: 0.009181\n",
      "Train Epoch: 8 [4640/8940 (52%)]\tLoss: 0.009280\n",
      "Train Epoch: 8 [4800/8940 (54%)]\tLoss: 0.009026\n",
      "Train Epoch: 8 [4960/8940 (55%)]\tLoss: 0.008754\n",
      "Train Epoch: 8 [5120/8940 (57%)]\tLoss: 0.008070\n",
      "Train Epoch: 8 [5280/8940 (59%)]\tLoss: 0.008132\n",
      "Train Epoch: 8 [5440/8940 (61%)]\tLoss: 0.008533\n",
      "Train Epoch: 8 [5600/8940 (63%)]\tLoss: 0.008855\n",
      "Train Epoch: 8 [5760/8940 (64%)]\tLoss: 0.008348\n",
      "Train Epoch: 8 [5920/8940 (66%)]\tLoss: 0.008647\n",
      "Train Epoch: 8 [6080/8940 (68%)]\tLoss: 0.008481\n",
      "Train Epoch: 8 [6240/8940 (70%)]\tLoss: 0.008771\n",
      "Train Epoch: 8 [6400/8940 (72%)]\tLoss: 0.009774\n",
      "Train Epoch: 8 [6560/8940 (73%)]\tLoss: 0.008299\n",
      "Train Epoch: 8 [6720/8940 (75%)]\tLoss: 0.008738\n",
      "Train Epoch: 8 [6880/8940 (77%)]\tLoss: 0.008131\n",
      "Train Epoch: 8 [7040/8940 (79%)]\tLoss: 0.008154\n",
      "Train Epoch: 8 [7200/8940 (81%)]\tLoss: 0.008125\n",
      "Train Epoch: 8 [7360/8940 (82%)]\tLoss: 0.007842\n",
      "Train Epoch: 8 [7520/8940 (84%)]\tLoss: 0.008462\n",
      "Train Epoch: 8 [7680/8940 (86%)]\tLoss: 0.009171\n",
      "Train Epoch: 8 [7840/8940 (88%)]\tLoss: 0.008657\n",
      "Train Epoch: 8 [8000/8940 (89%)]\tLoss: 0.009208\n",
      "Train Epoch: 8 [8160/8940 (91%)]\tLoss: 0.008796\n",
      "Train Epoch: 8 [8320/8940 (93%)]\tLoss: 0.008975\n",
      "Train Epoch: 8 [8480/8940 (95%)]\tLoss: 0.009691\n",
      "Train Epoch: 8 [8640/8940 (97%)]\tLoss: 0.008705\n",
      "Train Epoch: 8 [8800/8940 (98%)]\tLoss: 0.008373\n",
      "====> Epoch: 8 Average loss: 0.0088\n",
      "====> Test set loss: 0.0074\n",
      "Train Epoch: 9 [0/8940 (0%)]\tLoss: 0.008906\n",
      "Train Epoch: 9 [160/8940 (2%)]\tLoss: 0.008640\n",
      "Train Epoch: 9 [320/8940 (4%)]\tLoss: 0.008548\n",
      "Train Epoch: 9 [480/8940 (5%)]\tLoss: 0.008217\n",
      "Train Epoch: 9 [640/8940 (7%)]\tLoss: 0.008176\n",
      "Train Epoch: 9 [800/8940 (9%)]\tLoss: 0.008832\n",
      "Train Epoch: 9 [960/8940 (11%)]\tLoss: 0.008937\n",
      "Train Epoch: 9 [1120/8940 (13%)]\tLoss: 0.008748\n",
      "Train Epoch: 9 [1280/8940 (14%)]\tLoss: 0.010112\n",
      "Train Epoch: 9 [1440/8940 (16%)]\tLoss: 0.008491\n",
      "Train Epoch: 9 [1600/8940 (18%)]\tLoss: 0.008569\n",
      "Train Epoch: 9 [1760/8940 (20%)]\tLoss: 0.009823\n",
      "Train Epoch: 9 [1920/8940 (21%)]\tLoss: 0.008118\n",
      "Train Epoch: 9 [2080/8940 (23%)]\tLoss: 0.008596\n",
      "Train Epoch: 9 [2240/8940 (25%)]\tLoss: 0.009388\n",
      "Train Epoch: 9 [2400/8940 (27%)]\tLoss: 0.008880\n",
      "Train Epoch: 9 [2560/8940 (29%)]\tLoss: 0.009245\n",
      "Train Epoch: 9 [2720/8940 (30%)]\tLoss: 0.009039\n",
      "Train Epoch: 9 [2880/8940 (32%)]\tLoss: 0.009673\n",
      "Train Epoch: 9 [3040/8940 (34%)]\tLoss: 0.008939\n",
      "Train Epoch: 9 [3200/8940 (36%)]\tLoss: 0.008694\n",
      "Train Epoch: 9 [3360/8940 (38%)]\tLoss: 0.008887\n",
      "Train Epoch: 9 [3520/8940 (39%)]\tLoss: 0.008526\n",
      "Train Epoch: 9 [3680/8940 (41%)]\tLoss: 0.008755\n",
      "Train Epoch: 9 [3840/8940 (43%)]\tLoss: 0.009083\n",
      "Train Epoch: 9 [4000/8940 (45%)]\tLoss: 0.009384\n",
      "Train Epoch: 9 [4160/8940 (47%)]\tLoss: 0.008532\n",
      "Train Epoch: 9 [4320/8940 (48%)]\tLoss: 0.008254\n",
      "Train Epoch: 9 [4480/8940 (50%)]\tLoss: 0.009111\n",
      "Train Epoch: 9 [4640/8940 (52%)]\tLoss: 0.009271\n",
      "Train Epoch: 9 [4800/8940 (54%)]\tLoss: 0.008902\n",
      "Train Epoch: 9 [4960/8940 (55%)]\tLoss: 0.008630\n",
      "Train Epoch: 9 [5120/8940 (57%)]\tLoss: 0.007914\n",
      "Train Epoch: 9 [5280/8940 (59%)]\tLoss: 0.008271\n",
      "Train Epoch: 9 [5440/8940 (61%)]\tLoss: 0.008413\n",
      "Train Epoch: 9 [5600/8940 (63%)]\tLoss: 0.008762\n",
      "Train Epoch: 9 [5760/8940 (64%)]\tLoss: 0.008278\n",
      "Train Epoch: 9 [5920/8940 (66%)]\tLoss: 0.008572\n",
      "Train Epoch: 9 [6080/8940 (68%)]\tLoss: 0.008296\n",
      "Train Epoch: 9 [6240/8940 (70%)]\tLoss: 0.008614\n",
      "Train Epoch: 9 [6400/8940 (72%)]\tLoss: 0.009826\n",
      "Train Epoch: 9 [6560/8940 (73%)]\tLoss: 0.008303\n",
      "Train Epoch: 9 [6720/8940 (75%)]\tLoss: 0.008639\n",
      "Train Epoch: 9 [6880/8940 (77%)]\tLoss: 0.008072\n",
      "Train Epoch: 9 [7040/8940 (79%)]\tLoss: 0.008078\n",
      "Train Epoch: 9 [7200/8940 (81%)]\tLoss: 0.008064\n",
      "Train Epoch: 9 [7360/8940 (82%)]\tLoss: 0.007968\n",
      "Train Epoch: 9 [7520/8940 (84%)]\tLoss: 0.008380\n",
      "Train Epoch: 9 [7680/8940 (86%)]\tLoss: 0.009148\n",
      "Train Epoch: 9 [7840/8940 (88%)]\tLoss: 0.008474\n",
      "Train Epoch: 9 [8000/8940 (89%)]\tLoss: 0.009373\n",
      "Train Epoch: 9 [8160/8940 (91%)]\tLoss: 0.008998\n",
      "Train Epoch: 9 [8320/8940 (93%)]\tLoss: 0.009012\n",
      "Train Epoch: 9 [8480/8940 (95%)]\tLoss: 0.009796\n",
      "Train Epoch: 9 [8640/8940 (97%)]\tLoss: 0.008622\n",
      "Train Epoch: 9 [8800/8940 (98%)]\tLoss: 0.008314\n",
      "====> Epoch: 9 Average loss: 0.0088\n",
      "====> Test set loss: 0.0074\n",
      "Train Epoch: 10 [0/8940 (0%)]\tLoss: 0.008867\n",
      "Train Epoch: 10 [160/8940 (2%)]\tLoss: 0.008583\n",
      "Train Epoch: 10 [320/8940 (4%)]\tLoss: 0.008534\n",
      "Train Epoch: 10 [480/8940 (5%)]\tLoss: 0.008213\n",
      "Train Epoch: 10 [640/8940 (7%)]\tLoss: 0.008079\n",
      "Train Epoch: 10 [800/8940 (9%)]\tLoss: 0.008930\n",
      "Train Epoch: 10 [960/8940 (11%)]\tLoss: 0.008759\n",
      "Train Epoch: 10 [1120/8940 (13%)]\tLoss: 0.008737\n",
      "Train Epoch: 10 [1280/8940 (14%)]\tLoss: 0.010278\n",
      "Train Epoch: 10 [1440/8940 (16%)]\tLoss: 0.008611\n",
      "Train Epoch: 10 [1600/8940 (18%)]\tLoss: 0.008499\n",
      "Train Epoch: 10 [1760/8940 (20%)]\tLoss: 0.009704\n",
      "Train Epoch: 10 [1920/8940 (21%)]\tLoss: 0.008173\n",
      "Train Epoch: 10 [2080/8940 (23%)]\tLoss: 0.008532\n",
      "Train Epoch: 10 [2240/8940 (25%)]\tLoss: 0.009303\n",
      "Train Epoch: 10 [2400/8940 (27%)]\tLoss: 0.009056\n",
      "Train Epoch: 10 [2560/8940 (29%)]\tLoss: 0.009451\n",
      "Train Epoch: 10 [2720/8940 (30%)]\tLoss: 0.009041\n",
      "Train Epoch: 10 [2880/8940 (32%)]\tLoss: 0.009749\n",
      "Train Epoch: 10 [3040/8940 (34%)]\tLoss: 0.008919\n",
      "Train Epoch: 10 [3200/8940 (36%)]\tLoss: 0.008676\n",
      "Train Epoch: 10 [3360/8940 (38%)]\tLoss: 0.008895\n",
      "Train Epoch: 10 [3520/8940 (39%)]\tLoss: 0.008539\n",
      "Train Epoch: 10 [3680/8940 (41%)]\tLoss: 0.008632\n",
      "Train Epoch: 10 [3840/8940 (43%)]\tLoss: 0.009138\n",
      "Train Epoch: 10 [4000/8940 (45%)]\tLoss: 0.009452\n",
      "Train Epoch: 10 [4160/8940 (47%)]\tLoss: 0.008462\n",
      "Train Epoch: 10 [4320/8940 (48%)]\tLoss: 0.008249\n",
      "Train Epoch: 10 [4480/8940 (50%)]\tLoss: 0.009172\n",
      "Train Epoch: 10 [4640/8940 (52%)]\tLoss: 0.009216\n",
      "Train Epoch: 10 [4800/8940 (54%)]\tLoss: 0.008917\n",
      "Train Epoch: 10 [4960/8940 (55%)]\tLoss: 0.008638\n",
      "Train Epoch: 10 [5120/8940 (57%)]\tLoss: 0.007918\n",
      "Train Epoch: 10 [5280/8940 (59%)]\tLoss: 0.008025\n",
      "Train Epoch: 10 [5440/8940 (61%)]\tLoss: 0.008532\n",
      "Train Epoch: 10 [5600/8940 (63%)]\tLoss: 0.008689\n",
      "Train Epoch: 10 [5760/8940 (64%)]\tLoss: 0.008104\n",
      "Train Epoch: 10 [5920/8940 (66%)]\tLoss: 0.008549\n",
      "Train Epoch: 10 [6080/8940 (68%)]\tLoss: 0.008162\n",
      "Train Epoch: 10 [6240/8940 (70%)]\tLoss: 0.008715\n",
      "Train Epoch: 10 [6400/8940 (72%)]\tLoss: 0.009762\n",
      "Train Epoch: 10 [6560/8940 (73%)]\tLoss: 0.008186\n",
      "Train Epoch: 10 [6720/8940 (75%)]\tLoss: 0.008650\n",
      "Train Epoch: 10 [6880/8940 (77%)]\tLoss: 0.008159\n",
      "Train Epoch: 10 [7040/8940 (79%)]\tLoss: 0.008082\n",
      "Train Epoch: 10 [7200/8940 (81%)]\tLoss: 0.008106\n",
      "Train Epoch: 10 [7360/8940 (82%)]\tLoss: 0.007839\n",
      "Train Epoch: 10 [7520/8940 (84%)]\tLoss: 0.008216\n",
      "Train Epoch: 10 [7680/8940 (86%)]\tLoss: 0.009188\n",
      "Train Epoch: 10 [7840/8940 (88%)]\tLoss: 0.008497\n",
      "Train Epoch: 10 [8000/8940 (89%)]\tLoss: 0.009373\n",
      "Train Epoch: 10 [8160/8940 (91%)]\tLoss: 0.008769\n",
      "Train Epoch: 10 [8320/8940 (93%)]\tLoss: 0.008973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [8480/8940 (95%)]\tLoss: 0.009726\n",
      "Train Epoch: 10 [8640/8940 (97%)]\tLoss: 0.008639\n",
      "Train Epoch: 10 [8800/8940 (98%)]\tLoss: 0.008273\n",
      "====> Epoch: 10 Average loss: 0.0088\n",
      "====> Test set loss: 0.0074\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "\n",
    "        # 64 sets of random ZDIMS-float vectors\n",
    "        sample = Variable(torch.randn(64, ZDIMS))\n",
    "        if CUDA:\n",
    "            sample = sample.cuda()\n",
    "        sample = model.decode(sample).cpu()\n",
    "\n",
    "        # save out as an 8x8 matrix of generated problems\n",
    "        # this will give you a visual idea of how well latent space can generate things\n",
    "        # that look like problems\n",
    "        save_image(sample.data.view(64, 1, 18, 11),'reconstruction' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show an image of a generated problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = Variable(torch.randn(1, ZDIMS))\n",
    "if CUDA:\n",
    "    sample = sample.cuda()\n",
    "sample = model.decode(sample).cpu()\n",
    "\n",
    "        \n",
    "        # this will give you a visual idea of how well latent space can generate things\n",
    "        # that look like moonboard problems\n",
    "save_image(sample.data.view(1, 1, 18, 11),'test' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = sample.data[0].view(18,11).numpy()\n",
    "pic2 = np.flipud(pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKcAAAD4CAYAAACJzvbOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAKKklEQVR4nO3dbYxcZRnG8f9la6NFDCV9EWlrqykklRCRlaDEF6iQgoTyQZOSYKqSNDEB0ahYwge+Nkp8SSSaBipNRIhBkMYgtKmiMdHKbi1CaZEGsSytdImJGk2sDbcf5lR2t7PdzjnPzNy7c/2SZt7OztzTXHnmzHnmfo4iArOM3tTvAsym4nBaWg6npeVwWloOp6U1t5cvtnDhwlixYkUvX/KURkZGOtr+4osv7urz13mNmW5kZOS1iFjU7jH18lDS0NBQDA8P9+z1piOpo+07/b/q9PnrvMZMJ2kkIobaPeaPdUurUTglrZX0vKSDkjaVKsoMGoRT0hzgbuBqYDVwg6TVpQozazJyXgIcjIgXI+IY8CCwrkxZZs3CeS7w8rjbo9V9E0jaKGlY0vDY2FiDl7NB0ySc7b6KnvRVMyK2RMRQRAwtWtT2iIFZW03COQosG3d7KXC4WTlmb2gSzqeAVZJWSpoHrAe2lynLrMEMUUQcl3Qz8AQwB9gaEfuKVWYDr9H0ZUQ8BjxWqBazCXo6t55Nt6cKB20qsjRPX1paDqel5XBaWg6npeVwWloOp6XlcFpaDqel5XBaWg6npeVwWloDPbc+G3S7vbmfPHJaWg6npdWkNXiZpF9K2i9pn6RbSxZm1mSf8zjw5YjYI+lMYETSzoh4rlBtNuBqj5wRcSQi9lTX/wnsp01rsFldRfY5Ja0ALgJ2l3g+MygQTklvA34CfDEi/tHmcS+qYLU0XcjrzbSCeX9EPNxuGy+qYHU1+bYu4F5gf0R8s1xJZi1NRs7LgE8DV0jaW/27plBdZo0WVfgN7ddLMivCc+sz3EyaK++Upy8tLYfT0nI4LS2H09JyOC0th9PScjgtLYfT0nI4LS2H09JyOC0th9PScjgtLYfT0nI4La0SDW5zJP1B0s9KFGR2QomR81ZaPetmRTXtvlwKfAK4p0w5Zm9oOnJ+G7gNeH2qDdy3bnU1aQ2+FjgaESOn2s5961ZX09bg6yS9BDxIq0X4h0WqMqPZQl63R8TSiFgBrAd+ERE3FqvMBp6Pc1paRfrWI+JJ4MkSz2V2gkdOS8vhtLQcTkvL4bS0HE5Ly+G0tBxOS8vhtLQcTkvL4bS0HE5Ly+G0tHzCggHTOn3U6evnCRE8clpaDqel1bT78ixJD0k6IGm/pA+WKsys6T7nd4DHI+KTkuYB8wvUZAY0CKektwMfAT4DEBHHgGNlyjJr9rH+bmAM+EG1HM09ks6YvJH71q2uJuGcC7wf+F5EXAT8C9g0eSP3rVtdTcI5CoxGxO7q9kO0wmpWRJO+9b8CL0s6v7prDfBckarMaP5t/Rbg/uqb+ovAZ5uXZNbSKJwRsRcYKlSL2QSeWx8w/Zwr75SnLy0th9PScjgtLYfT0nI4LS2H09JyOC0th9PScjgtLYfT0nI4LS2H09JyOC0th9PSatq3/iVJ+yQ9K+kBSW8pVZhZkxOzngt8ARiKiAuAObROM2hWRNOP9bnAWyXNpbWgwuHmJZm1NGlwewW4CzgEHAH+HhE7Jm/nvnWrq8nH+gJgHbASeCdwhqSTzhrsvnWrq8nH+seBP0fEWET8F3gY+FCZssyahfMQcKmk+WqtSLoG2F+mLLNm+5y7aa3ysQd4pnquLYXqMmvct34ncGehWswm8AyRpeVwWloOp6XlcFpaDqel5XBaWg6npeVwWloOp6XlcFpaDqel5XBaWl4TPpmZdD70bvPIaWk5nJbWtOGUtFXSUUnPjrvvbEk7Jb1QXS7obpk2iE5n5LwPWDvpvk3ArohYBeyizQlZzZqaNpwR8Wvgb5PuXgdsq65vA64vXJdZ7X3OJRFxBKC6XDzVhu5bt7q6/oXIfetWV91wvirpHIDq8mi5ksxa6oZzO7Chur4BeLRMOWZvOJ1DSQ8AvwXOlzQq6SZgM3ClpBeAK6vbZkVNO30ZETdM8dCawrWYTeC59WQ6nSufzXPxnr60tBxOS8vhtLQcTkvL4bS0HE5Ly+G0tBxOS8vhtLQcTkvL4bS0PLfeRZ3Oe0Pnc98zaa68Ux45LS2H09Kq27f+DUkHJP1R0iOSzupumTaI6vat7wQuiIgLgT8Btxeuy6xe33pE7IiI49XN3wFLu1CbDbgS+5yfA34+1YPuW7e6GoVT0h3AceD+qbZx37rVVfs4p6QNwLXAmpjNB9usb2qFU9Ja4GvARyPi32VLMmup27f+XeBMYKekvZK+3+U6bQDV7Vu/twu1mE3gufUu8q54M56+tLQcTkvL4bS0HE5Ly+G0tBxOS8vhtLQcTkvL4bS0HE5Ly+G0tBxOS8vhtLQcTkurVt/6uMe+IikkLexOeTbI6vatI2kZrbO3HSpckxlQ/3zrAN8CbgP8i1rrilr7nJKuA16JiKcL12P2fx23aUiaD9wBXHWa228ENgIsX76805ezAVZn5HwPsBJ4WtJLtJai2SPpHe029qIKVlfHI2dEPAMsPnG7CuhQRLxWsC6z2n3rZl3X5HzrJx5fUawas3E8Q2RpeVGFGa7TkyLMpIUePHJaWg6npeVwWloOp6XlcFpaDqel5XBaWg6npeVwWloOp6XlcFpanluf4WbSXHmnPHJaWg6npVV7UQVJt0h6XtI+SV/vXok2qGotqiDpcmAdcGFEvBe4q3xpNujqLqrweWBzRPyn2uZoF2qzAVd3n/M84MOSdkv6laQPTLWhpI2ShiUNj42N1Xw5G0R1wzkXWABcCnwV+LGm6Bdw37rVVTeco8DD0fJ74HXAK81ZUXXD+VPgCgBJ5wHzAC+qYEVNO0NULarwMWChpFHgTmArsLU6vHQM2BCzearC+qLJogo3Fq7FbALPEFlaDqel5XBaWg6npeVwWloOp6XlcFpaDqel5XBaWg6npeVwWloOp6WlXv6YSNIY8Jc2Dy1ksH5yN2jvF6Z+z++KiLa/Qu9pOKciaTgihvpdR68M2vuFeu/ZH+uWlsNpaWUJ55Z+F9Bjg/Z+ocZ7TrHPadZOlpHT7CQOp6XV13BKWlstBnZQ0qZ+1tIrkl6S9IykvZKG+11PN7Rb/E3S2ZJ2Snqhulww3fP0LZyS5gB3A1cDq4EbJK3uVz09dnlEvG8WH+u8j0mLvwGbgF0RsQrYVd0+pX6OnJcAByPixYg4BjxIa+U6m+GmWPxtHbCtur4NuH665+lnOM8FXh53e7S6b7YLYIekEUkb+11MDy2JiCMA1eXi6f6gn2vCt1v4axCOa10WEYclLQZ2SjpQjTQ2ST9HzlFg2bjbS4HDfaqlZyLicHV5FHiE1u7NIHhV0jkA1eW0a7r2M5xPAaskrZQ0D1gPbO9jPV0n6QxJZ564DlwFPHvqv5o1tgMbqusbgEen+4O+faxHxHFJNwNPAHOArRGxr1/19MgS4JFqKdO5wI8i4vH+llTeFIu/baa1jutNwCHgU9M+j6cvLSvPEFlaDqel5XBaWg6npeVwWloOp6XlcFpa/wM2Qda8JI519QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(pic2>0.21, cmap='gray_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
